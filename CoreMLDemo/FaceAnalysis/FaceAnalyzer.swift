import Foundation
import Vision
import UIKit

class FaceAnalyzer: ObservableObject {
    @Published var currentImage: UIImage?
    @Published var detectedFaces: [VNFaceObservation] = []
    @Published var isProcessing = false
    @Published var errorMessage: String?
    
    enum Emotion: String {
        case happy = "ğŸ˜Š"
        case sad = "ğŸ˜¢"
        case angry = "ğŸ˜ "
        case neutral = "ğŸ˜"
        case surprise = "ğŸ˜²"
    }
    
    func analyzeImage(_ image: UIImage) {
        self.currentImage = image
        self.isProcessing = true
        self.errorMessage = nil
        self.detectedFaces = []
        
        guard let cgImage = image.cgImage else {
            self.errorMessage = "æ— æ³•å¤„ç†å›¾ç‰‡"
            self.isProcessing = false
            return
        }
        
        let request = VNDetectFaceLandmarksRequest { [weak self] request, error in
            DispatchQueue.main.async {
                if let error = error {
                    self?.errorMessage = "åˆ†æå¤±è´¥: \(error.localizedDescription)"
                    self?.isProcessing = false
                    return
                }
                
                guard let observations = request.results as? [VNFaceObservation] else {
                    self?.errorMessage = "æœªæ£€æµ‹åˆ°äººè„¸"
                    self?.isProcessing = false
                    return
                }
                
                self?.detectedFaces = observations
                self?.isProcessing = false
            }
        }
        
        let handler = VNImageRequestHandler(cgImage: cgImage, options: [:])
        do {
            try handler.perform([request])
        } catch {
            DispatchQueue.main.async {
                self.errorMessage = "åˆ†æå¤±è´¥: \(error.localizedDescription)"
                self.isProcessing = false
            }
        }
    }
    
    func determineEmotion(for face: VNFaceObservation) -> Emotion {
        guard let landmarks = face.landmarks else {
            return .neutral
        }
        
        if let mouth = landmarks.outerLips?.normalizedPoints,
           let leftBrow = landmarks.leftEyebrow?.normalizedPoints,
           let rightBrow = landmarks.rightEyebrow?.normalizedPoints,
           let leftEye = landmarks.leftEye?.normalizedPoints,
           let rightEye = landmarks.rightEye?.normalizedPoints {
            
            // è®¡ç®—å˜´éƒ¨ç‰¹å¾
            let mouthHeight = mouth.map { $0.y }.max()! - mouth.map { $0.y }.min()!
            let mouthWidth = mouth.map { $0.x }.max()! - mouth.map { $0.x }.min()!
            let mouthAspectRatio = mouthHeight / mouthWidth
            
            // è®¡ç®—å˜´è§’å¼§åº¦
            let mouthCorners = [mouth[0], mouth[mouth.count/2]] 
            let mouthAngle = atan2(mouthCorners[1].y - mouthCorners[0].y,
                                 mouthCorners[1].x - mouthCorners[0].x)
            
            // è®¡ç®—çœ‰æ¯›é«˜åº¦
            let leftBrowHeight = leftBrow.map { $0.y }.reduce(0, +) / Double(leftBrow.count)
            let rightBrowHeight = rightBrow.map { $0.y }.reduce(0, +) / Double(rightBrow.count)
            let avgBrowHeight = (leftBrowHeight + rightBrowHeight) / 2
            
            // è®¡ç®—çœ¼ç›å¼€åˆåº¦
            let leftEyeHeight = leftEye.map { $0.y }.max()! - leftEye.map { $0.y }.min()!
            let rightEyeHeight = rightEye.map { $0.y }.max()! - rightEye.map { $0.y }.min()!
            let avgEyeHeight = (leftEyeHeight + rightEyeHeight) / 2
            
            // å¾®ç¬‘åˆ¤æ–­ï¼šå˜´è§’ä¸Šæ‰¬ï¼Œé€‚å½“çš„å®½é«˜æ¯”
            if mouthAngle > 0.05 && mouthAspectRatio < 0.35 && mouthWidth > 0.3 {
                return .happy
            }
            
            // æ‚²ä¼¤åˆ¤æ–­ï¼šå˜´è§’ä¸‹å‚ï¼Œçœ‰æ¯›ä½å‚
            if mouthAngle < -0.05 && avgBrowHeight < 0.65 && mouthWidth < 0.35 {
                return .sad
            }
            
            // ç”Ÿæ°”åˆ¤æ–­ï¼šçœ‰æ¯›ä½å‚ï¼Œå˜´å·´ç´§é—­
            if avgBrowHeight < 0.55 && mouthHeight < 0.15 && mouthWidth < 0.3 {
                return .angry
            }
            
            // æƒŠè®¶åˆ¤æ–­ï¼šçœ‰æ¯›ä¸Šæ‰¬ï¼Œå˜´å·´å¼ å¼€ï¼Œçœ¼ç›çå¤§
            if avgBrowHeight > 0.65 && mouthHeight > 0.25 && avgEyeHeight > 0.2 {
                return .surprise
            }
        }
        
        return .neutral
    }
    
    func drawFaceAnnotations(on image: UIImage) -> UIImage {
        guard let cgImage = image.cgImage else { return image }
        
        let imageSize = CGSize(width: CGFloat(cgImage.width),
                             height: CGFloat(cgImage.height))
        
        UIGraphicsBeginImageContextWithOptions(imageSize, false, 0.0)
        let context = UIGraphicsGetCurrentContext()
        
        let imageRect = CGRect(origin: .zero, size: imageSize)
        context?.translateBy(x: 0, y: imageSize.height)
        context?.scaleBy(x: 1.0, y: -1.0)
        context?.draw(cgImage, in: imageRect)
        context?.scaleBy(x: 1.0, y: -1.0)
        context?.translateBy(x: 0, y: -imageSize.height)
        
        for face in detectedFaces {
            let faceRect = VNImageRectForNormalizedRect(face.boundingBox,
                                                       Int(imageSize.width),
                                                       Int(imageSize.height))
            // æ›´ç²¾ç¡®åœ°è°ƒæ•´äººè„¸æ¡†çš„ä½ç½®
            let adjustedFaceRect = CGRect(x: faceRect.minX,
                                        y: faceRect.minY - faceRect.height * 0.15, // è°ƒæ•´ä¸ºå‘ä¸Šç§»åŠ¨15%çš„é«˜åº¦
                                        width: faceRect.width,
                                        height: faceRect.height * 0.95) // ç¨å¾®å‡å°é«˜åº¦ä»¥ä½¿æ¡†æ›´è´´åˆé¢éƒ¨
            
            context?.setStrokeColor(UIColor.green.cgColor)
            context?.setLineWidth(3)
            context?.addRect(adjustedFaceRect)
            context?.strokePath()
            
            let emotion = determineEmotion(for: face)
            let emoji = emotion.rawValue
            let attributes: [NSAttributedString.Key: Any] = [
                .font: UIFont.systemFont(ofSize: adjustedFaceRect.width * 0.3),
                .foregroundColor: UIColor.white
            ]
            let emojiSize = (emoji as NSString).size(withAttributes: attributes)
            let emojiPoint = CGPoint(x: adjustedFaceRect.minX + (adjustedFaceRect.width - emojiSize.width) / 2,
                                   y: adjustedFaceRect.minY - emojiSize.height - 2) // å°†emojiç¨å¾®é è¿‘æ¡†
            
            (emoji as NSString).draw(at: emojiPoint, withAttributes: attributes)
        }
        
        let annotatedImage = UIGraphicsGetImageFromCurrentImageContext()
        UIGraphicsEndImageContext()
        
        return annotatedImage ?? image
    }
}
